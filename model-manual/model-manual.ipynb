{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, LSTM, Dense, Dropout\n",
    "\n",
    "from keras.utils import to_categorical\n",
    "from keras import backend as K\n",
    "\n",
    "from sklearn.base import TransformerMixin\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.ensemble import RandomForestClassifier \n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.ensemble import StackingClassifier\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.pipeline import Pipeline\n",
    "import numpy as np\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from itertools import product\n",
    "import matplotlib.pyplot as plt\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomTokenizer(TransformerMixin):\n",
    "    def __init__(self) -> None:\n",
    "        self.tokenizer = Tokenizer()\n",
    "        \n",
    "    def tokenize(self, data: list) -> list:\n",
    "        X = self.tokenizer.texts_to_sequences(data)\n",
    "        X = pad_sequences(X, maxlen=self.max_length)\n",
    "        return X\n",
    "    \n",
    "    def tokenize_df(self, df: pd.DataFrame) -> list:\n",
    "        return self.tokenize(df['tweet'].tolist())\n",
    "    \n",
    "    def fit(self, X, y=None):\n",
    "        self.data = X['tweet'].tolist()\n",
    "        self.tokenizer.fit_on_texts(self.data)\n",
    "        self.vocab_size = len(self.tokenizer.word_index) + 1\n",
    "        self.data_tok = self.tokenizer.texts_to_sequences(self.data)\n",
    "        self.max_length = max(len(seq) for seq in self.data_tok)\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X):\n",
    "        return self.tokenize_df(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_dataset(filename: str) -> tuple[pd.DataFrame, pd.DataFrame]:\n",
    "    df = pd.read_csv(filename)\n",
    "    y = df.loc[:, 'class']\n",
    "    X = df.drop(['class'], axis=1)\n",
    "    return X, y\n",
    "\n",
    "def save_data_to_csv(data, path):\n",
    "    f = open(path, mode='w')\n",
    "    data.to_csv(f, index=False)\n",
    "    f.close()\n",
    "\n",
    "def searchMaxRowByColumn(dataframe, columnName):\n",
    "    return dataframe[dataframe[columnName] == dataframe[columnName].max()].iloc[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, y_train = read_dataset(\"../Data/train_data.csv\")\n",
    "X_test, y_test = read_dataset(\"../Data/test_data.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = CustomTokenizer()\n",
    "tokenizer.fit(X_train)\n",
    "# y_train_cat = to_categorical(y_train, num_classes=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_list_1 = [\n",
    "    ('rf1', RandomForestClassifier(n_estimators=15, max_depth=2, random_state=42)),\n",
    "    ('gb1', GradientBoostingClassifier(n_estimators=15, max_depth=2,learning_rate=0.1, random_state=42)),\n",
    "    ('sv1', SVC(C=1.0, kernel='rbf', gamma='scale', random_state=42)),\n",
    "    ('kn1', KNeighborsClassifier(n_neighbors=5)),\n",
    "    ('dt1', DecisionTreeClassifier(max_depth=2, random_state=42)),\n",
    "    ('mlp1',MLPClassifier(solver='lbfgs', alpha=1e-5, hidden_layer_sizes=(50, ), random_state=42)),\n",
    "]\n",
    "\n",
    "model_list_2 = [\n",
    "    ('rf2', RandomForestClassifier(n_estimators=15, max_depth=2, random_state=42)),\n",
    "    ('gb2', GradientBoostingClassifier(n_estimators=15, max_depth=2, random_state=42)),\n",
    "    ('sv2', SVC(C=1.0, kernel='rbf', gamma='scale', random_state=42)),\n",
    "    ('kn2', KNeighborsClassifier(n_neighbors=5, )),\n",
    "    ('dt2', DecisionTreeClassifier(max_depth=2, random_state=42)),\n",
    "    ('mlp2', MLPClassifier(solver='lbfgs', alpha=1e-5, hidden_layer_sizes=(50, ), random_state=42)),\n",
    "]\n",
    "\n",
    "all_model_combinations = list(product(model_list_1, model_list_2))\n",
    "formatted_model_combinations = [[(name1, model1), (name2, model2)] for ((name1, model1), (name2, model2)) in all_model_combinations]\n",
    "\n",
    "stack = StackingClassifier(estimators=[], final_estimator=LogisticRegression())\n",
    "pipeline = Pipeline([\n",
    "    ('tokenizer', CustomTokenizer()),\n",
    "    ('stack', stack)\n",
    "])\n",
    "\n",
    "param_grid = {\n",
    "    'stack__estimators':formatted_model_combinations\n",
    "}\n",
    "\n",
    "grid_search = GridSearchCV(\n",
    "    estimator=pipeline,\n",
    "    param_grid=param_grid,\n",
    "    n_jobs=-1,\n",
    "    verbose=1,\n",
    "    cv=2,\n",
    "    scoring='balanced_accuracy'\n",
    "    )\n",
    "\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "cs_results = pd.DataFrame(grid_search.cv_results_)\n",
    "save_data_to_csv(cs_results, f\"stack_test.csv\")\n",
    "\n",
    "analisys_final_best = pd.read_csv('stack_test.csv')\n",
    "\n",
    "print(\"\\nNajlepszy znaleziony model:\\n\")\n",
    "print(searchMaxRowByColumn(analisys_final_best,'mean_test_score'))\n",
    "# najlepszy gb1 i dt2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wykonajmy tuning hiperparametrów:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Wartości zmiennych \n",
    "n_jobs=-1\n",
    "n_iter_randomSearch=250\n",
    "verbose=1\n",
    "random_state=42\n",
    "cv=3\n",
    "numberOfPoints=10\n",
    "\n",
    "n_estimators_lower_gb = 5\n",
    "n_estimators_upper_gb = 50\n",
    "max_depth_lower_gb = 1\n",
    "max_depth_upper_gb = 8\n",
    "min_samples_split_lower_gb = 1\n",
    "min_samples_split_upper_gb = 10\n",
    "min_samples_leaf_lower_gb = 1\n",
    "min_samples_leaf_upper_gb = 5\n",
    "learning_rate_lower_gb = 0.05\n",
    "learning_rate_upper_gb = 0.5\n",
    "subsample_lower_gb = 0.75\n",
    "subsample_upper_gb = 1.0\n",
    "\n",
    "max_depth_lower_dt = 1\n",
    "max_depth_upper_dt = 10\n",
    "min_samples_split_lower_dt = 1\n",
    "min_samples_split_upper_dt = 5\n",
    "min_samples_leaf_lower_dt = 1\n",
    "min_samples_leaf_upper_dt = 10\n",
    "max_features_dt = ['sqrt', 'log2']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stack = StackingClassifier(estimators=[\n",
    "        ('gb1', GradientBoostingClassifier(random_state=42)),\n",
    "        ('dt1', DecisionTreeClassifier(random_state=42)),\n",
    "    ], final_estimator=LogisticRegression())\n",
    "pipeline = Pipeline([\n",
    "        ('tokenizer', CustomTokenizer()),\n",
    "        ('model', stack)\n",
    "    ])\n",
    "\n",
    "param_dist = {\n",
    "    'model__gb1__n_estimators': np.linspace(n_estimators_lower_gb, n_estimators_upper_gb, n_estimators_upper_gb - n_estimators_lower_gb).astype(int),\n",
    "    'model__gb1__max_depth': np.linspace(max_depth_lower_gb, max_depth_upper_gb, max_depth_upper_gb- max_depth_lower_gb).astype(int),\n",
    "    'model__gb1__min_samples_split': np.linspace(min_samples_split_lower_gb, min_samples_split_upper_gb, min_samples_split_upper_gb - min_samples_split_lower_gb).astype(int),\n",
    "    'model__gb1__min_samples_leaf': np.linspace(min_samples_leaf_lower_gb, min_samples_leaf_upper_gb, min_samples_leaf_upper_gb - min_samples_leaf_lower_gb).astype(int),\n",
    "    'model__gb1__learning_rate': np.linspace(learning_rate_lower_gb, learning_rate_upper_gb, numberOfPoints).astype(float),\n",
    "    'model__gb1__subsample': np.linspace(subsample_lower_gb, subsample_upper_gb, numberOfPoints).astype(float),\n",
    "    'model__dt1__max_depth': np.linspace(max_depth_lower_dt, max_depth_upper_dt, max_depth_upper_dt- max_depth_lower_dt).astype(int),\n",
    "    'model__dt1__min_samples_split': np.linspace(min_samples_split_lower_dt, min_samples_split_upper_dt, min_samples_split_upper_dt - min_samples_split_lower_dt).astype(int),\n",
    "    'model__dt1__min_samples_leaf': np.linspace(min_samples_leaf_lower_dt, min_samples_leaf_upper_dt, min_samples_leaf_upper_dt - min_samples_leaf_lower_dt).astype(int),\n",
    "    'model__dt1__max_features': max_features_dt,\n",
    "    }\n",
    "\n",
    "random_search = RandomizedSearchCV(\n",
    "        estimator=pipeline,\n",
    "        param_distributions=param_dist,\n",
    "        n_jobs=n_jobs,\n",
    "        n_iter=n_iter_randomSearch,\n",
    "        verbose=verbose,\n",
    "        random_state=random_state,\n",
    "        cv=cv,\n",
    "        scoring='balanced_accuracy'\n",
    "        )\n",
    "\n",
    "random_search.fit(X_train, y_train)\n",
    "\n",
    "cs_ran_results = pd.DataFrame(random_search.cv_results_)\n",
    "save_data_to_csv(cs_ran_results, f\"stack_tuning.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wybór najlepszego modelu:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "analisys_final_best = pd.read_csv('stack_tuning.csv')\n",
    "\n",
    "print(\"\\nNajlepszy znaleziony model:\\n\")\n",
    "print(searchMaxRowByColumn(analisys_final_best,'mean_test_score'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stack = StackingClassifier(estimators=[\n",
    "        ('gb1', GradientBoostingClassifier(n_estimators=43, subsample=0.9444444444444444, min_samples_leaf=2, min_samples_split=8, max_depth=8, learning_rate=0.35, random_state=42)),\n",
    "        ('dt1', DecisionTreeClassifier(max_depth=6, max_features=\"sqrt\", min_samples_leaf=4, min_samples_split=2, random_state=42)),\n",
    "    ], final_estimator=LogisticRegression())\n",
    "pipeline = Pipeline([\n",
    "        ('tokenizer', CustomTokenizer()),\n",
    "        ('stack', stack)\n",
    "    ])\n",
    "pipeline.fit(X_train, y_train)\n",
    "y_pred = pipeline.predict(X_test)\n",
    "\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "report = classification_report(y_test, y_pred)\n",
    "\n",
    "y_pred_dt = pd.DataFrame(y_pred)\n",
    "\n",
    "print(\"Accuracy for test dataset:\", accuracy)\n",
    "print(\"Classification Report:\")\n",
    "print(report)\n",
    "\n",
    "y_pred_train = pipeline.predict(X_train)\n",
    "\n",
    "accuracy = accuracy_score(y_train, y_pred_train)\n",
    "report = classification_report(y_train, y_pred_train)\n",
    "\n",
    "y_pred_train_dt = pd.DataFrame(y_pred_train)\n",
    "\n",
    "print(\"Accuracy for train dataset:\", accuracy)\n",
    "print(\"Classification Report:\")\n",
    "print(report)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analiza FP i FN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 0 -> ok\n",
    "# 1 -> hate\n",
    "\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "tn,fp,fn,tp = cm.ravel()\n",
    "print(tn, fp, fn, tp)\n",
    "\n",
    "print(\"FP:\", fp / (fp + fn) * 100, \"%\")\n",
    "print(\"FN:\", fn / (fp + fn) * 100, \"%\")\n",
    "\n",
    "incorrect_idx = y_pred != y_test\n",
    "incorrect = X_test[incorrect_idx]\n",
    "incorrect['len'] = incorrect['tweet'].apply(len)\n",
    "\n",
    "plt.hist(incorrect['len'], bins=10, color='skyblue', edgecolor='black')\n",
    "plt.xlabel('Długość tweeta')\n",
    "plt.ylabel('Liczba tweetów')\n",
    "plt.title('Histogram długości tweetów sklasyfikowanych błędnie\\nprzez model zbudowany od podstaw')\n",
    "plt.savefig('hist-incorrect-manual.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
