{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, LSTM, Dense, Dropout\n",
    "\n",
    "from keras.utils import to_categorical\n",
    "from keras import backend as K\n",
    "\n",
    "from sklearn.base import TransformerMixin\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.ensemble import RandomForestClassifier \n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.ensemble import StackingClassifier\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.pipeline import Pipeline\n",
    "import numpy as np\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from itertools import product"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomTokenizer(TransformerMixin):\n",
    "    def __init__(self) -> None:\n",
    "        self.tokenizer = Tokenizer()\n",
    "        \n",
    "    def tokenize(self, data: list) -> list:\n",
    "        X = self.tokenizer.texts_to_sequences(data)\n",
    "        X = pad_sequences(X, maxlen=self.max_length)\n",
    "        return X\n",
    "    \n",
    "    def tokenize_df(self, df: pd.DataFrame) -> list:\n",
    "        return self.tokenize(df['tweet'].tolist())\n",
    "    \n",
    "    def fit(self, X, y=None):\n",
    "        self.data = X['tweet'].tolist()\n",
    "        self.tokenizer.fit_on_texts(self.data)\n",
    "        self.vocab_size = len(self.tokenizer.word_index) + 1\n",
    "        self.data_tok = self.tokenizer.texts_to_sequences(self.data)\n",
    "        self.max_length = max(len(seq) for seq in self.data_tok)\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X):\n",
    "        return self.tokenize_df(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_dataset(filename: str) -> tuple[pd.DataFrame, pd.DataFrame]:\n",
    "    df = pd.read_csv(filename)\n",
    "    y = df.loc[:, 'class']\n",
    "    X = df.drop(['class'], axis=1)\n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, y_train = read_dataset(\"../Data/train_data.csv\")\n",
    "X_test, y_test = read_dataset(\"../Data/test_data.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<__main__.CustomTokenizer at 0x1dad54673d0>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer = CustomTokenizer()\n",
    "tokenizer.fit(X_train)\n",
    "# y_train_cat = to_categorical(y_train, num_classes=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_list_1 = [\n",
    "    ('rf1', RandomForestClassifier(n_estimators=15, max_depth=2, random_state=42)),\n",
    "    ('gb1', GradientBoostingClassifier(n_estimators=15, max_depth=2,learning_rate=0.1, random_state=42)),\n",
    "    ('sv1', SVC(C=1.0, kernel='rbf', gamma='scale', random_state=42)),\n",
    "    ('kn1', KNeighborsClassifier(n_neighbors=5)),\n",
    "    ('dt1', DecisionTreeClassifier(max_depth=2, random_state=42)),\n",
    "    ('mlp1',MLPClassifier(solver='lbfgs', alpha=1e-5, hidden_layer_sizes=(50, ), random_state=42)),\n",
    "]\n",
    "\n",
    "model_list_2 = [\n",
    "    ('rf2', RandomForestClassifier(n_estimators=15, max_depth=2, random_state=42)),\n",
    "    ('gb2', GradientBoostingClassifier(n_estimators=15, max_depth=2, random_state=42)),\n",
    "    ('sv2', SVC(C=1.0, kernel='rbf', gamma='scale', random_state=42)),\n",
    "    ('kn2', KNeighborsClassifier(n_neighbors=5, )),\n",
    "    ('dt2', DecisionTreeClassifier(max_depth=2, random_state=42)),\n",
    "    ('mlp2', MLPClassifier(solver='lbfgs', alpha=1e-5, hidden_layer_sizes=(50, ), random_state=42)),\n",
    "]\n",
    "\n",
    "all_model_combinations = list(product(model_list_1, model_list_2))\n",
    "formatted_model_combinations = [[(name1, model1), (name2, model2)] for ((name1, model1), (name2, model2)) in all_model_combinations]\n",
    "\n",
    "stack = StackingClassifier(estimators=[], final_estimator=LogisticRegression())\n",
    "pipeline = Pipeline([\n",
    "    ('tokenizer', CustomTokenizer()),\n",
    "    ('stack', stack)\n",
    "])\n",
    "\n",
    "param_grid = {\n",
    "    'stack__estimators':formatted_model_combinations\n",
    "}\n",
    "\n",
    "grid_search = GridSearchCV(\n",
    "    estimator=pipeline,\n",
    "    param_grid=param_grid,\n",
    "    n_jobs=-1,\n",
    "    verbose=1,\n",
    "    cv=2,\n",
    "    scoring='balanced_accuracy'\n",
    "    )\n",
    "\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "cs_results = pd.DataFrame(grid_search.cv_results_)\n",
    "\n",
    "# najlepszy gb1 i dt2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'cs_results' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[8], line 6\u001b[0m\n\u001b[0;32m      3\u001b[0m     data\u001b[38;5;241m.\u001b[39mto_csv(f, index\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[0;32m      4\u001b[0m     f\u001b[38;5;241m.\u001b[39mclose()\n\u001b[1;32m----> 6\u001b[0m save_data_to_csv(\u001b[43mcs_results\u001b[49m, \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstack_test.csv\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'cs_results' is not defined"
     ]
    }
   ],
   "source": [
    "def save_data_to_csv(data, path):\n",
    "    f = open(path, mode='w')\n",
    "    data.to_csv(f, index=False)\n",
    "    f.close()\n",
    "\n",
    "save_data_to_csv(cs_results, f\"stack_test.csv\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wykonajmy tuning hiperparametrów:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Wartości zmiennych \n",
    "n_jobs=-1\n",
    "n_iter_randomSearch=250\n",
    "verbose=1\n",
    "random_state=42\n",
    "cv=3\n",
    "numberOfPoints=10\n",
    "\n",
    "n_estimators_lower_gb = 5\n",
    "n_estimators_upper_gb = 50\n",
    "max_depth_lower_gb = 1\n",
    "max_depth_upper_gb = 8\n",
    "min_samples_split_lower_gb = 1\n",
    "min_samples_split_upper_gb = 10\n",
    "min_samples_leaf_lower_gb = 1\n",
    "min_samples_leaf_upper_gb = 5\n",
    "learning_rate_lower_gb = 0.05\n",
    "learning_rate_upper_gb = 0.5\n",
    "subsample_lower_gb = 0.75\n",
    "subsample_upper_gb = 1.0\n",
    "\n",
    "max_depth_lower_dt = 1\n",
    "max_depth_upper_dt = 10\n",
    "min_samples_split_lower_dt = 1\n",
    "min_samples_split_upper_dt = 5\n",
    "min_samples_leaf_lower_dt = 1\n",
    "min_samples_leaf_upper_dt = 10\n",
    "max_features_dt = ['sqrt', 'log2']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 250 candidates, totalling 750 fits\n"
     ]
    }
   ],
   "source": [
    "stack = StackingClassifier(estimators=[\n",
    "        ('gb1', GradientBoostingClassifier(random_state=42)),\n",
    "        ('dt1', DecisionTreeClassifier(random_state=42)),\n",
    "    ], final_estimator=LogisticRegression())\n",
    "pipeline = Pipeline([\n",
    "        ('tokenizer', CustomTokenizer()),\n",
    "        ('model', stack)\n",
    "    ])\n",
    "\n",
    "param_dist = {\n",
    "    'model__gb1__n_estimators': np.linspace(n_estimators_lower_gb, n_estimators_upper_gb, n_estimators_upper_gb - n_estimators_lower_gb).astype(int),\n",
    "    'model__gb1__max_depth': np.linspace(max_depth_lower_gb, max_depth_upper_gb, max_depth_upper_gb- max_depth_lower_gb).astype(int),\n",
    "    'model__gb1__min_samples_split': np.linspace(min_samples_split_lower_gb, min_samples_split_upper_gb, min_samples_split_upper_gb - min_samples_split_lower_gb).astype(int),\n",
    "    'model__gb1__min_samples_leaf': np.linspace(min_samples_leaf_lower_gb, min_samples_leaf_upper_gb, min_samples_leaf_upper_gb - min_samples_leaf_lower_gb).astype(int),\n",
    "    'model__gb1__learning_rate': np.linspace(learning_rate_lower_gb, learning_rate_upper_gb, numberOfPoints).astype(float),\n",
    "    'model__gb1__subsample': np.linspace(subsample_lower_gb, subsample_upper_gb, numberOfPoints).astype(float),\n",
    "    'model__dt1__max_depth': np.linspace(max_depth_lower_dt, max_depth_upper_dt, max_depth_upper_dt- max_depth_lower_dt).astype(int),\n",
    "    'model__dt1__min_samples_split': np.linspace(min_samples_split_lower_dt, min_samples_split_upper_dt, min_samples_split_upper_dt - min_samples_split_lower_dt).astype(int),\n",
    "    'model__dt1__min_samples_leaf': np.linspace(min_samples_leaf_lower_dt, min_samples_leaf_upper_dt, min_samples_leaf_upper_dt - min_samples_leaf_lower_dt).astype(int),\n",
    "    'model__dt1__max_features': max_features_dt,\n",
    "    }\n",
    "\n",
    "random_search = RandomizedSearchCV(\n",
    "        estimator=pipeline,\n",
    "        param_distributions=param_dist,\n",
    "        n_jobs=n_jobs,\n",
    "        n_iter=n_iter_randomSearch,\n",
    "        verbose=verbose,\n",
    "        random_state=random_state,\n",
    "        cv=cv,\n",
    "        scoring='balanced_accuracy'\n",
    "        )\n",
    "\n",
    "random_search.fit(X_train, y_train)\n",
    "\n",
    "cs_ran_results = pd.DataFrame(random_search.cv_results_)\n",
    "save_data_to_csv(cs_ran_results, f\"stack_tuning.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wybór najlepszego modelu:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Najlepszy znaleziony model:\n",
      "\n",
      "mean_fit_time                                                                  72.741037\n",
      "std_fit_time                                                                    0.697991\n",
      "mean_score_time                                                                 0.349859\n",
      "std_score_time                                                                  0.010388\n",
      "param_model__gb1__subsample                                                     0.944444\n",
      "param_model__gb1__n_estimators                                                        43\n",
      "param_model__gb1__min_samples_split                                                    8\n",
      "param_model__gb1__min_samples_leaf                                                     2\n",
      "param_model__gb1__max_depth                                                            8\n",
      "param_model__gb1__learning_rate                                                     0.35\n",
      "param_model__dt1__min_samples_split                                                    2\n",
      "param_model__dt1__min_samples_leaf                                                     4\n",
      "param_model__dt1__max_features                                                      sqrt\n",
      "param_model__dt1__max_depth                                                            6\n",
      "params                                 {'model__gb1__subsample': 0.9444444444444444, ...\n",
      "split0_test_score                                                               0.864797\n",
      "split1_test_score                                                               0.862512\n",
      "split2_test_score                                                                0.86767\n",
      "mean_test_score                                                                 0.864993\n",
      "std_test_score                                                                   0.00211\n",
      "rank_test_score                                                                        1\n",
      "Name: 235, dtype: object\n"
     ]
    }
   ],
   "source": [
    "def searchMaxRowByColumn(dataframe, columnName):\n",
    "    return dataframe[dataframe[columnName] == dataframe[columnName].max()].iloc[0]\n",
    "\n",
    "analisys_final_best = pd.read_csv('stack_tuning.csv')\n",
    "\n",
    "print(\"\\nNajlepszy znaleziony model:\\n\")\n",
    "print(searchMaxRowByColumn(analisys_final_best,'mean_test_score'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy for test dataset: 0.867869222096956\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.85      0.90      0.87      4438\n",
      "           1       0.89      0.84      0.86      4432\n",
      "\n",
      "    accuracy                           0.87      8870\n",
      "   macro avg       0.87      0.87      0.87      8870\n",
      "weighted avg       0.87      0.87      0.87      8870\n",
      "\n",
      "Accuracy for train dataset: 0.9464456846496421\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.92      0.97      0.95     17736\n",
      "           1       0.97      0.92      0.95     17742\n",
      "\n",
      "    accuracy                           0.95     35478\n",
      "   macro avg       0.95      0.95      0.95     35478\n",
      "weighted avg       0.95      0.95      0.95     35478\n",
      "\n"
     ]
    }
   ],
   "source": [
    "stack = StackingClassifier(estimators=[\n",
    "        ('gb1', GradientBoostingClassifier(n_estimators=43, subsample=0.9444444444444444, min_samples_leaf=2, min_samples_split=8, max_depth=8, learning_rate=0.35, random_state=42)),\n",
    "        ('dt1', DecisionTreeClassifier(max_depth=6, max_features=\"sqrt\", min_samples_leaf=4, min_samples_split=2, random_state=42)),\n",
    "    ], final_estimator=LogisticRegression())\n",
    "pipeline = Pipeline([\n",
    "        ('tokenizer', CustomTokenizer()),\n",
    "        ('stack', stack)\n",
    "    ])\n",
    "pipeline.fit(X_train, y_train)\n",
    "y_pred = pipeline.predict(X_test)\n",
    "\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "report = classification_report(y_test, y_pred)\n",
    "\n",
    "y_pred_dt = pd.DataFrame(y_pred)\n",
    "\n",
    "print(\"Accuracy for test dataset:\", accuracy)\n",
    "print(\"Classification Report:\")\n",
    "print(report)\n",
    "\n",
    "y_pred_train = pipeline.predict(X_train)\n",
    "\n",
    "accuracy = accuracy_score(y_train, y_pred_train)\n",
    "report = classification_report(y_train, y_pred_train)\n",
    "\n",
    "y_pred_train_dt = pd.DataFrame(y_pred_train)\n",
    "\n",
    "print(\"Accuracy for train dataset:\", accuracy)\n",
    "print(\"Classification Report:\")\n",
    "print(report)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
