{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "W tym dokumencie zostały poruszone następujące kwestie: \n",
    "1. Krótka analiza ekploracyjna danych \n",
    "2. Preprocessing danych \n",
    "3. Podział danych na zbiór testowy i zbiór treningowy  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import re # regex to detect username, url, html entity \n",
    "import nltk # to use word tokenize (split the sentence into words)\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Podczas analizy różnych zbiorów danych zdecydowano się na użycie zbiorów danych, które nie zostały wygenerowane, a rekordy faktycznie pochodzą z twittera. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wczytywanie danych: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_1 = pd.read_csv(\"./labeled_data.csv\")\n",
    "data_2 = pd.read_csv(\"./train_E6oV3lV.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sprawdzenie braków danych: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(data_1.isnull().sum())\n",
    "print(data_2.isnull().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wybieranie wspólnych kolumn i zmiana nazwy kolumn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_1 = data_1[['tweet', 'class']]\n",
    "data_2 = data_2[['tweet', 'label']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_1_counts = data_1['class'].value_counts()\n",
    "print(\"Liczba rekordów dla danych klas (pierwszy zbiór):\")\n",
    "print(data_1_counts)\n",
    "\n",
    "data_2_counts = data_2['label'].value_counts()\n",
    "print(\"\\nLiczba rekordów dla danych klas (drugi zbiór):\")\n",
    "print(data_2_counts)\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "data_1_counts.plot(kind='bar')\n",
    "plt.title('Liczba rekordów dla danych klas (pierwszy zbiór)')\n",
    "plt.xlabel('Klasa')\n",
    "plt.ylabel('Liczba rekordów')\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "data_2_counts.plot(kind='bar')\n",
    "plt.title('Liczba rekordów dla danych klas (drugi zbiór)')\n",
    "plt.xlabel('Klasa')\n",
    "plt.ylabel('Liczba rekordów')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Zgodnie z dokumentacją dostępną na stronie:  \n",
    "https://www.kaggle.com/datasets/mrmorj/hate-speech-and-offensive-language-dataset\n",
    "\n",
    "klasy oznaczają:\n",
    "- 0 - hate speech\n",
    "- 1 - offensive language \n",
    "- 2 - neighter\n",
    "\n",
    "https://www.kaggle.com/datasets/vkrahul/twitter-hate-speech\n",
    "\n",
    "klasy oznaczają:\n",
    "- 0 - no hate speech \n",
    "- 1 - hate speech\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Można zauważyć, że oddzielnie te zbiory danych nie są zbyt wiele warte, ze względu na swoje zbilansowanie, jednakże połączenie tych dwóch zbiorów danych sprawi, że będziemy mogli mówić o wiarygodnym uczeniu maszynowym. Możemy przyjąć, że offensive language będzie traktowane jako hate speech w naszym projekcie."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_1['class'] = data_1['class'].replace({0: 1, 2: 0})\n",
    "data_2 = data_2.rename(columns={'label': 'class'})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Połączenie obu zbiorów danych:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_data = pd.concat([data_1, data_2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = set(stopwords.words('english'))\n",
    "stop_words.add(\"rt\")\n",
    "\n",
    "def remove_entity(raw_text):\n",
    "    entity_regex = r\"&[^\\s;]+;\"\n",
    "    text = re.sub(entity_regex, \"\", raw_text)\n",
    "    return text\n",
    "\n",
    "def change_user(raw_text):\n",
    "    regex_1 = r\"@([^ ]+)|@ *([^ ]+)\"\n",
    "    text = re.sub(regex_1, \"user\", raw_text)\n",
    "    text = text.replace(\"@\", '')\n",
    "    return text\n",
    "\n",
    "def remove_url(raw_text):\n",
    "    url_regex = r\"(?i)\\b((?:https?:?//|www\\d{0,3}[.]|[a-z0-9.\\-]+[.][a-z]{2,4}/)(?:[^\\s()<>]+|\\(([^\\s()<>]+|(\\([^\\s()<>]+\\)))*\\))+(?:\\(([^\\s()<>]+|(\\([^\\s()<>]+\\)))*\\)|[^\\s`!()\\[\\]{};:'\\\".,<>?«»“”‘’]))\"\n",
    "    text = re.sub(url_regex, '', raw_text)\n",
    "    url_regex = r\"htt([^ ]+)\"\n",
    "    text = re.sub(url_regex, '', raw_text)\n",
    "    return text\n",
    "\n",
    "def remove_noise_symbols(raw_text):\n",
    "    text = raw_text.replace('\"', '')\n",
    "    text = text.replace(\"'\", '')\n",
    "    text = text.replace(\"!\", '')\n",
    "    text = text.replace(\"`\", '')\n",
    "    text = text.replace(\"..\", '')\n",
    "    text = text.replace(\"*\", '')\n",
    "    text = text.replace(\"-\", '')\n",
    "    text = text.replace(\":\", '')\n",
    "    text = text.replace(\"_\", '')\n",
    "    text = text.replace(\"=\", '')\n",
    "    text = text.replace(\"^\", '')\n",
    "    text = text.replace(\"$\", '')\n",
    "    text = text.replace(\"|\", '')\n",
    "    text = text.replace(\"~\", '')\n",
    "    text = text.replace(\"\\\\\", '')\n",
    "    text = re.sub(r\"\\[.*?\\]\", \"\", text)\n",
    "    text = re.sub(r\"/\", \" / \", text)\n",
    "    return text\n",
    "\n",
    "def remove_stopwords(raw_text):\n",
    "    tokenize = nltk.word_tokenize(raw_text)\n",
    "    text = [word for word in tokenize if not word.lower() in stop_words]\n",
    "    text = \" \".join(text)\n",
    "\n",
    "    return text\n",
    "\n",
    "def preprocess(datas, exclude_chars):\n",
    "    clean = []\n",
    "    clean = [change_user(text) for text in datas]\n",
    "    clean = [remove_entity(text) for text in clean]\n",
    "    clean = [remove_url(text) for text in clean]\n",
    "    clean = [remove_noise_symbols(text) for text in clean]\n",
    "    clean = [remove_stopwords(text) for text in clean]\n",
    "    clean = [text.lower() for text in clean]\n",
    "    for char in exclude_chars:\n",
    "        clean = [text.replace(char, '') for text in clean]\n",
    "\n",
    "    return clean"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Został wygenerowany słownik znaków występujących w zbiorach danych, które zdecydowano się usuną"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "exclude_chars = {'ð', '\\x9f', '\\x93', '±', '\\x98', '\\x99', '\\x8e', '\\x91', '\\x84', '\\x92', '¦', 'â', '\\x80', '¯', '\\xad', '\\x86', '\\x9d', '\\x90', '\\x9c', '\\x88', '\\x8a', '¤', 'ï', '¸', '\\x8f', '\\x8c', '¼', '»', '¶', '\\x89', '\\x82', '\\x81', '\\x83', '\\x8d', '½', '©', '\\x9b', '³', '\\x96', '¥', 'ª', '\\x9a', 'º', '\\x95', '\\x9e', 'ó', '¾', '®', '¨', 'ã', '\\x8b', '\\x87', '«', '\\x97', '£', '\\x94', '¿', '·', 'æ', '´', 'å', 'ç', 'ä', '¹', '¢', 'µ', '²', '¬', '°', 'ì', 'ë', 'à', 'ê', 'í', 'é', '§', '¡', 'è', 'î', 'ø', 'ù', '[', 'á', '×', 'ñ', 'ò'}\n",
    "\n",
    "tweets = list(merged_data['tweet'])\n",
    "labels = list(merged_data['class'])\n",
    "\n",
    "clean_tweets = preprocess(tweets, exclude_chars)\n",
    "clean_data = pd.DataFrame({'tweet': clean_tweets, 'class': labels})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Ilość rekordów przed redukcją duplikatów: \",len(clean_data))\n",
    "clean_data = clean_data.drop_duplicates()\n",
    "print(\"Ilość rekordów po redukcji duplikatów: \",len(clean_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_data_counts = clean_data['class'].value_counts()\n",
    "print(\"Liczba rekordów dla danych klas (oczyszczony zbiór):\")\n",
    "print(clean_data_counts)\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "clean_data_counts.plot(kind='bar')\n",
    "plt.title('Liczba rekordów dla danych klas (oczyszczony zbiór)')\n",
    "plt.xlabel('Klasa')\n",
    "plt.ylabel('Liczba rekordów')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_data['tweet_length'] = clean_data['tweet'].apply(lambda x: len(x))\n",
    "max_tweet_length = clean_data['tweet_length'].max()\n",
    "print(\"Długość najdłuższego tweeta:\", max_tweet_length)\n",
    "\n",
    "plt.hist(clean_data['tweet_length'], bins=30, color='skyblue', edgecolor='black')\n",
    "plt.xlabel('Długość tweeta')\n",
    "plt.ylabel('Liczba tweety')\n",
    "plt.title('Histogram długości tweetów')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Można zauważyć, że zbiory są nadal dostatecznie zbilansowane. Dlatego zdecydowano się na undersampling klasy 0, biorąc pod uwagę długość tweeta."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted_clean_data = clean_data.sort_values(by=['class', 'tweet_length'])\n",
    "\n",
    "class_0 = sorted_clean_data[(sorted_clean_data['class'] == 0)].head(len(clean_data[clean_data['class'] == 1]))\n",
    "class_1 = sorted_clean_data[sorted_clean_data['class'] == 1]\n",
    "\n",
    "balanced_clean_data = pd.concat([class_0, class_1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(balanced_clean_data['tweet_length'], bins=30, color='skyblue', edgecolor='black')\n",
    "plt.xlabel('Długość tweeta')\n",
    "plt.ylabel('Liczba tweety')\n",
    "plt.title('Histogram długości tweetów po zbalansowaniu klas')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "balanced_clean_data_counts = balanced_clean_data['class'].value_counts()\n",
    "print(\"Liczba rekordów dla danych klas (oczyszczony zbilansowany zbiór):\")\n",
    "print(balanced_clean_data_counts)\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "balanced_clean_data_counts.plot(kind='bar')\n",
    "plt.title('Liczba rekordów dla danych klas (oczyszczony zbalansowany zbiór)')\n",
    "plt.xlabel('Klasa')\n",
    "plt.ylabel('Liczba rekordów')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Zapisanie zbioru danych przed podziałem do pliku CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "balanced_clean_data = balanced_clean_data.drop(columns=['tweet_length'])\n",
    "balanced_clean_data.to_csv(\"dataset_before_split.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wykonajmy podział danych i zapiszmy je do plików."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = balanced_clean_data.drop(columns=['class'])\n",
    "y = balanced_clean_data['class'] \n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = pd.concat([X_train, y_train], axis=1)  \n",
    "train_data.to_csv('train_data.csv', index=False)  \n",
    "\n",
    "test_data = pd.concat([X_test, y_test], axis=1) \n",
    "test_data.to_csv('test_data.csv', index=False)  "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
